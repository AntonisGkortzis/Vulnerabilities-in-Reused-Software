{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# JSS'19 - Gkortzis et al. - Data Collection\n",
    "\n",
    "This notebook describes and performs the following steps necessary to collect all data used in the study:\n",
    "\n",
    "1. [Requirements](#requirements)\n",
    "1. [Download projects from GitHub](#download)\n",
    "2. [Detect Maven root directories](#detect_root_dirs)\n",
    "3. [Compile and install Maven projects](#install)\n",
    "4. [Retrieve projects' dependencies](#dependencies)\n",
    "5. [Run Spotbugs](#spotbugs)\n",
    "6. [Extract metrics and create analysis dataset](#metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"requirements\"></a>\n",
    "## Requirements\n",
    "\n",
    "The external tools required to run the analysis can be automatically obtained by executing the ```download-vendor-tools.sh``` script in the __analysis/tooling__ directory. The follwoing runtime environments and tools shoudl also be available in your system: \n",
    "1. Open-jdk 8 & open-jdk 11 (some projects can be build with version 1.8 only)\n",
    "2. Python3\n",
    "3. Unzip\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"download\"></a>\n",
    "## Download Projects from GitHub\n",
    "\n",
    "With a list of Github Maven repositories, the first step is to clone these reposirotories locally. This is the task that the github_downloader script performs. \n",
    "\n",
    "The execute_downloader requires three parameters:\n",
    "1. `credentials` : `String`. The github credentials in a specific format (`github_username:github_token`)\n",
    "2. `repository_list` : `Filepath`. The list of repositories to clone locally\n",
    "3. `download_directory` : `Directory`. The fullpath of the directory where the repositories will be cloned\n",
    "Additionally, there is an optional parameter: \n",
    "4. `Boolean`. That can be used to update (perform a git pull) on an already existing repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import github_downloader\n",
    "\n",
    "credentials = \"github_username:github_token\" # replace this value with your personal credentials\n",
    "repository_list = \"../maven_starred_sorted_all.csv\"\n",
    "download_directory = \"/media/agkortzis/Data/maven_repos\" # replace this value\n",
    "\n",
    "github_downloader.execute_downloader(credentials, repository_list, download_directory)\n",
    "# github_downloader.execute_downloader(credentials, repoInputFile, repoStoreRoot, update_existing=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"detect_root_dirs\"></a>\n",
    "## Detect Maven root directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import repository_path_retriever\n",
    "\n",
    "repositories_root_directories_file = \"../repositories_root_directories.csv\" # the file that will store the repositories with their detected maven root direcotry\n",
    "repositories_alternative_directories_file = \"../repositories_alternative_directories_file.csv\" # the file that stores the paths for repositories with more than one maven parent projects\n",
    "repository_path_retriever.detect_configuration_file(download_directory,repositories_root_directories_file,repositories_alternative_directories_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"install\"></a>\n",
    "## Compile and install Maven projects\n",
    "\n",
    "The next step after downloading the projects is to perform a maven install. This process will collect the projects dependencies and generate the `.jar` file. Both, the `.jar` file and the dependencies will be stored in the `.m2` directory under each project's direcoty. \n",
    "This `.m2` root directory is, by default, located under the users folder (`/home/user/.m2`).\n",
    "\n",
    "The `execute_mvn_install` requires one argument:\n",
    "1. `root_directory` : `Directory`. The full path of the directory in which all repositories (downloaded by the previous step) are located.\n",
    "\n",
    "Users can also define the following two optional parametes:\n",
    "2. `Boolean`. Perform a mvn clean on each repository before compiling it again, and\n",
    "3. `Datetime`. Checkout each repository to a specific date. Date shoud be formatted as `YYYY-MM-DD` (example `2018-12-25`) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import project_installer\n",
    "\n",
    "root_directory = download_directory # replace this value\n",
    "repositories_sucessfully_build_list = \"../sucessfully_built_repositories.csv\" # \n",
    "project_installer.install_all_repositories(root_directory,repositories_root_directories_file,repositories_sucessfully_build_list)\n",
    "#project_installer.install_all_repositories(root_directory, repository_list, build_list_file, clean_repository_before_install, skip_maven, skip_gradle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"dependencies\"></a>\n",
    "## Retrieve project dependencies\n",
    "\n",
    "Having a local copy of each maven reporitory we proceed with retrieving their dependency tree. Each tree will be stored in a separate file (with `.trees` suffix) for further analysis as we describe on the next [step](#spotbugs). If a project consist of more than one modules, a seperate tree of each module will be stored in the `.trees` file.\n",
    "\n",
    "This step requires two parameters:\n",
    "1. `root_directory` : `Directory`. The full path of the directory that stores the repositories \n",
    "2. `output_directory` : `Directory`. The full path of the directory that will store the `.trees` files. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import dependency_extractor\n",
    "\n",
    "os.chdir('/home/agkortzis/git_repos/ICSR19/analysis/tooling')\n",
    "os.getcwd()\n",
    "\n",
    "root_directory = f\"/media/agkortzis/Data/maven_repos/\" # replace this value\n",
    "output_directory = \"/home/agkortzis/git_repos/ICSR19/analysis/data/\" # replace this value\n",
    "#repositories_sucessfully_build_list = repositories_sucessfull_build_list # from step \"Compile and install Maven projects\"\n",
    "repositories_sucessfully_build_list = \"../successfuly_built_maven_repos_part2.txt\" # from step \"Compile and install Maven projects\"\n",
    "repositories_root_directories_file = \"../downloaded_repos_with_maven_rootpaths.txt\" # from step \"Detect Maven root directories\"\n",
    "\n",
    "dependency_extractor.execute(root_directory, output_directory, repositories_sucessfully_build_list, repositories_root_directories_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"spotbugs\"></a>\n",
    "## Run SpotBugs\n",
    "\n",
    "With the installed projects, the next step is to run SpotBugs.\n",
    "For that, we use the `.trees` files, which contain the dependency tree for each module built for the project.\n",
    "\n",
    "Thus, for each project (i.e., each `.trees` files), the next sub-steps are:\n",
    "1. Parse the `.trees` file\n",
    "2. Ignore modules that are not Java source code (not `.jar` nor `.war`)\n",
    "3. For each remaining tree (i.e., for each `.jar`/`.war` module):\n",
    "  1. Select relevant dependencies (i.e., compile dependencies)\n",
    "  2. Verify if main package and dependencies are installed in the `.m2` local repository\n",
    "  3. Run SpotBugs for the set **[module] + [dependencies]**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import datetime\n",
    "\n",
    "import maven\n",
    "import spotbugs\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "os.chdir('/home/agkortzis/git_repos/ICSR19/analysis/tooling')\n",
    "os.getcwd()\n",
    "path_to_data = os.path.abspath('../repositories_data') #TODO: replace with relative\n",
    "path_to_m2_directory = '/media/agkortzis/Data/m2'\n",
    "\n",
    "def run_spotbugs(file_project_trees, output_file, path_to_m2_directory=os.path.expanduser('~/.m2')):\n",
    "\n",
    "    trees = maven.get_compiled_modules(file_project_trees)\n",
    "    \n",
    "    if not trees:\n",
    "        logging.info(f'No modules to analyze: {file_project_trees}.')\n",
    "        return\n",
    "\n",
    "    pkg_paths = []\n",
    "    for t in trees:\n",
    "        pkg_paths.extend([a.artifact.get_m2_path(path_to_m2_directory) for a in t])\n",
    "        \n",
    "    pkg_paths = list(set(pkg_paths))\n",
    "    spotbugs.analyze_project(pkg_paths, output_file)\n",
    "    \n",
    "    \n",
    "currentDT = datetime.datetime.now()\n",
    "print (\"Started at :: {}\".format(str(currentDT)))\n",
    "\n",
    "projects_tress = [f for f in os.listdir(path_to_data) if f.endswith('.trees')]\n",
    "\n",
    "counter = 1\n",
    "total = len(projects_tress)\n",
    "for f in projects_tress:\n",
    "    filepath = path_to_data + os.path.sep + f\n",
    "    output_file = f'{os.path.splitext(filepath)[0]}.xml'\n",
    "    logging.info(\"{}/{}\".format(counter,total))\n",
    "    run_spotbugs(filepath, output_file, path_to_m2_directory)\n",
    "    counter = counter + 1\n",
    "\n",
    "    \n",
    "currentDT = datetime.datetime.now()\n",
    "print (\"Finished at :: {}\".format(str(currentDT)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"metrics\"></a>\n",
    "## Extract metrics and create analysis dataset\n",
    "\n",
    "*[Describe steps]*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "import itertools\n",
    "import logging\n",
    "import datetime\n",
    "\n",
    "import maven as mvn\n",
    "import spotbugs as sb\n",
    "import sloc\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "def count_vulnerabilities(spotbugs_xml, classes_sloc, class_sets, class_dict): # {'uv': project_classes, 'dv': dep_classes}\n",
    "    vdict = sb.collect_vulnerabilities(spotbugs_xml, class_dict)\n",
    "    \n",
    "    dataset_info = {}\n",
    "    for k in class_dict.keys():\n",
    "        dataset_info[k] = {}\n",
    "        dataset_info[k][\"classes\"] = [sb.get_main_classname(b) for c in vdict[k] for r in c for b in r]\n",
    "        dataset_info[k][\"classes\"] = list(set(dataset_info[k][\"classes\"]))\n",
    "        dataset_info[k][\"count\"] = [len(r) for c in vdict[k] for r in c]\n",
    "        dataset_info[k][\"sloc\"] = 0\n",
    "        dataset_info[k][\"classes_sloc\"] = 0\n",
    "\n",
    "        try:\n",
    "            dataset_info[k][\"sloc\"] = sum([int(classes_sloc.get(c,0)) for c in sloc.get_roots(class_dict[k])])\n",
    "            dataset_info[k][\"classes_sloc\"] = sum([int(classes_sloc.get(c,0)) for c in sloc.get_roots(dataset_info[k][\"classes\"])])\n",
    "        except Exception as e:\n",
    "            with open ('/home/agkortzis/git_repos/ICSR19/analysis/log_dep_classes.txt', 'w') as log:\n",
    "                for entry in sloc.get_roots(class_dict[k]):\n",
    "                    log.write(\"{}\\n\".format(entry))\n",
    "                \n",
    "            with open ('/home/agkortzis/git_repos/ICSR19/analysis/log_dep_original.txt', 'w') as  log:\n",
    "                for entry in class_dict[k]:\n",
    "                    log.write(\"{}\\n\".format(entry))\n",
    "                \n",
    "            raise e\n",
    "            logging.error(\"Error while calculating metrics\\n{}\".format(e))#angor\n",
    "\n",
    "    \n",
    "    return [len(class_dict[k]) for k in class_sets] + [len(dataset_info[k][\"classes\"])  for k in class_sets] + [dataset_info[k][\"sloc\"] for k in class_sets] + [dataset_info[k][\"classes_sloc\"]  for k in class_sets] + [count for k in class_sets for count in dataset_info[k][\"count\"]] # e.g., #uv_p1_r1 | #uv_p1_r2 ... | #dv_p3_r3 | #dv_p3_r4\n",
    "\n",
    "\n",
    "def project_level_metrics(trees, spotbugs_xml, path_to_m2_directory=os.path.expanduser('~/.m2')):\n",
    "    modules = [m.artifact for m in trees]\n",
    "    dep_modules = [m.artifact for t in trees for m in t.deps if m.artifact not in modules]\n",
    "    dep_modules = list(set(dep_modules)) # remove duplicates\n",
    "    metrics = {}\n",
    "\n",
    "    # Collect SLOC info\n",
    "    classes_sloc = {}\n",
    "    for m in (modules + dep_modules):\n",
    "        classes_sloc.update(sloc.retrieve_SLOC(m.get_m2_path(path_to_m2_directory))[0])\n",
    "    \n",
    "    # Collect classes from user code\n",
    "    project_classes = [c for m in modules for c in m.get_class_list(path_to_m2_directory)]\n",
    "    \n",
    "    # Collect classes from dependencies\n",
    "    ## Original dataset\n",
    "    try:\n",
    "        dep_classes = [c for m in dep_modules for c in m.get_class_list(path_to_m2_directory)]\n",
    "    except Exception as e:#angor\n",
    "        with open ('/home/agkortzis/git_repos/ICSR19/analysis/log_modules.txt', 'w') as log:\n",
    "            log.write(\"{}\\n\".format(spotbugs_xml))\n",
    "            for entry in dep_modules:\n",
    "                log.write(\"{}\\n\".format(entry.get_m2_path(path_to_m2_directory)))\n",
    "        raise e\n",
    "    metrics['general'] = count_vulnerabilities(spotbugs_xml, classes_sloc, ['uv', 'dv'], {'uv': project_classes, 'dv': dep_classes})\n",
    "\n",
    "    ## Enterprise dataset (compare enterprise vs. non-enterprise dependencies)\n",
    "    dm_enterprise = [m for m in dep_modules if m.groupId in enterprise_group_ids]\n",
    "    dm_not_enterprise = [m for m in dep_modules if m.groupId not in enterprise_group_ids]\n",
    "    try:\n",
    "        dc_enterprise = [c for m in dm_enterprise for c in m.get_class_list(path_to_m2_directory)]\n",
    "        dc_not_enterprise = [c for m in dm_not_enterprise for c in m.get_class_list(path_to_m2_directory)]\n",
    "    except Exception as e:#angor\n",
    "        with open ('/home/agkortzis/git_repos/ICSR19/analysis/log_modules-enterprise.txt', 'w') as log:\n",
    "            log.write(\"{}\\n\".format(spotbugs_xml))\n",
    "            for entry in dep_modules:\n",
    "                log.write(\"{}\\n\".format(entry.get_m2_path(path_to_m2_directory)))\n",
    "        raise e\n",
    "    metrics['enterprise'] = count_vulnerabilities(spotbugs_xml, classes_sloc, ['uv', 'dve', 'dvne'], {'uv': project_classes, 'dve': dc_enterprise, 'dvne': dc_not_enterprise})\n",
    "\n",
    "    ## Well-known projects (compare well-known community projects vs. non-well-known projects dependencies)\n",
    "    dm_known = [m for m in dep_modules if m.groupId in wellknown_group_ids]\n",
    "    dm_not_known = [m for m in dep_modules if m.groupId not in wellknown_group_ids]\n",
    "    try:\n",
    "        dc_known = [c for m in dm_known for c in m.get_class_list(path_to_m2_directory)]\n",
    "        dc_not_known = [c for m in dm_not_known for c in m.get_class_list(path_to_m2_directory)]\n",
    "    except Exception as e:#angor\n",
    "        with open ('/home/agkortzis/git_repos/ICSR19/analysis/log_modules-wellknown.txt', 'w') as log:\n",
    "            log.write(\"{}\\n\".format(spotbugs_xml))\n",
    "            for entry in dep_modules:\n",
    "                log.write(\"{}\\n\".format(entry.get_m2_path(path_to_m2_directory)))\n",
    "        raise e\n",
    "    metrics['wellknown'] = count_vulnerabilities(spotbugs_xml, classes_sloc, ['uv', 'dvw', 'dvnw'], {'uv': project_classes, 'dvw': dc_known, 'dvnw': dc_not_known})\n",
    "\n",
    "    return metrics\n",
    "\n",
    "\n",
    "def collect_sp_metrics(file_project_trees, output_file, append_to_file=True, path_to_m2_directory=os.path.expanduser('~/.m2')):\n",
    "\n",
    "    trees = mvn.get_compiled_modules(file_project_trees)\n",
    "    spotbugs_xml = f'{os.path.splitext(file_project_trees)[0]}.xml'\n",
    "    proj_name = os.path.basename(os.path.splitext(file_project_trees)[0])\n",
    "    logging.info(\"Project :: {}\".format(proj_name))\n",
    "    \n",
    "    if not trees:\n",
    "        logging.warning(f'No modules to analyze: {file_project_trees}.')\n",
    "        return\n",
    "    \n",
    "    if not os.path.exists(spotbugs_xml):\n",
    "        logging.warning(f'SpotBugs XML not found: {spotbugs_xml}.')\n",
    "        return\n",
    "    \n",
    "    metrics = project_level_metrics(trees, spotbugs_xml, path_to_m2_directory)\n",
    "\n",
    "    for dataset in metrics.keys():\n",
    "        if append_to_file:\n",
    "            with open(output_file+dataset+'.csv', 'a') as f:\n",
    "                f.write(','.join([proj_name] + [str(m) for m in metrics[dataset]]) + os.linesep)\n",
    "        else:\n",
    "            with open(output_file+dataset+'.csv', 'w') as f:\n",
    "                f.write(','.join([proj_name] + [str(m) for m in metrics[dataset]]) + os.linesep)\n",
    "            \n",
    "        logging.debug(f'{dataset}||' + ','.join([proj_name] + [str(m) for m in metrics[dataset]]) + os.linesep)\n",
    "\n",
    "\n",
    "def create_headers(class_sets):\n",
    "    proj_info = [f'#{k}{h}' for h in ['_classes', 'v_classes', '_sloc', 'v_classes_sloc'] for k in class_sets]\n",
    "    vcount = [f'#{k}v_p{p}_r{r}' for k in class_sets for p in range(1,4) for r in range(1,5)]\n",
    "    return ['project'] + proj_info + vcount\n",
    "\n",
    "\n",
    "\n",
    "currentDT = datetime.datetime.now()\n",
    "print (\"Started at :: {}\".format(str(currentDT)))\n",
    "\n",
    "\n",
    "os.chdir('/home/agkortzis/git_repos/ICSR19/analysis/tooling')\n",
    "os.getcwd()\n",
    "path_to_m2_directory = '/media/agkortzis/2TB_EX_STEREO/m2'\n",
    "path_to_data = os.path.abspath('../repositories_data')\n",
    "projects_dataset = os.path.abspath('../jss_revised_dataset.csv')\n",
    "\n",
    "projects_escope_dataset = os.path.abspath('../dependencies_groupids_enterprise_info.csv')\n",
    "with open(projects_escope_dataset) as escope_csv:\n",
    "    enterprise_group_ids = set()\n",
    "    wellknown_group_ids = set()\n",
    "    rows = csv.reader(escope_csv, delimiter=';')\n",
    "    for r in rows:\n",
    "        if r[2] == '1':\n",
    "            enterprise_group_ids.add(r[1])\n",
    "#             logging.info(\"Entreprise id = {}\".format(r[1]))\n",
    "        if r[6] == '1':\n",
    "            wellknown_group_ids.add(r[1])\n",
    "#             logging.info(\"Well known id = {}\".format(r[1]))\n",
    "# \n",
    "\n",
    "with open(projects_dataset+'general.csv', 'w') as f:\n",
    "    class_sets = ['u', 'd']\n",
    "    f.write(','.join(create_headers(class_sets)) + os.linesep)\n",
    "\n",
    "with open(projects_dataset+'enterprise.csv', 'w') as f:\n",
    "    class_sets = ['u', 'de', 'dne']\n",
    "    f.write(','.join(create_headers(class_sets)) + os.linesep)\n",
    "\n",
    "with open(projects_dataset+'wellknown.csv', 'w') as f:\n",
    "    class_sets = ['u', 'dw', 'dnw']\n",
    "    f.write(','.join(create_headers(class_sets)) + os.linesep)\n",
    "    \n",
    "projects_tress = [f for f in os.listdir(path_to_data) if f.endswith('.trees')]\n",
    "\n",
    "number_of_projects = len(projects_tress)\n",
    "for index, f in enumerate(projects_tress):\n",
    "    logging.info(\"{}/{} --> {}\".format(index,number_of_projects,f))\n",
    "    filepath = path_to_data + os.path.sep + f\n",
    "    collect_sp_metrics(filepath, projects_dataset, path_to_m2_directory)\n",
    "\n",
    "currentDT = datetime.datetime.now()\n",
    "print (\"Finished at :: {}\".format(str(currentDT)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import itertools\n",
    "import logging\n",
    "import datetime\n",
    "\n",
    "import maven as mvn\n",
    "import spotbugs as sb\n",
    "import sloc\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "os.chdir('/home/agkortzis/git_repos/ICSR19/analysis/tooling')\n",
    "os.getcwd()\n",
    "path_to_m2_directory = '/media/agkortzis/2TB_EX_STEREO/m2'\n",
    "path_to_data = os.path.abspath('../repositories_data')\n",
    "projects_dataset = os.path.abspath('../jss_revised_dataset.csv')\n",
    "\n",
    "\n",
    "def project_level_metrics(trees, spotbugs_xml, path_to_m2_directory=os.path.expanduser('~/.m2')):\n",
    "    modules = [m.artifact for m in trees]\n",
    "    dep_modules = [m.artifact for t in trees for m in t.deps if m.artifact not in modules]\n",
    "    dep_modules = list(set(dep_modules)) # remove duplicates\n",
    "    \n",
    "    # Collect classes from user code\n",
    "    project_classes = [c for m in modules for c in m.get_class_list(path_to_m2_directory)]\n",
    "    \n",
    "    try:\n",
    "    # Collect classes from dependencies\n",
    "        dep_classes = [c for m in dep_modules for c in m.get_class_list(path_to_m2_directory)]\n",
    "    except Exception as e:#angor\n",
    "        with open ('/home/agkortzis/git_repos/ICSR19/analysis/log_modules.txt', 'w') as log:\n",
    "            log.write(\"{}\\n\".format(spotbugs_xml))\n",
    "            for entry in dep_modules:\n",
    "                log.write(\"{}\\n\".format(entry.get_m2_path(path_to_m2_directory)))\n",
    "        raise e\n",
    "                \n",
    "    # Collect SLOC info\n",
    "    classes_sloc = {}\n",
    "    for m in (modules + dep_modules):\n",
    "        classes_sloc.update(sloc.retrieve_SLOC(m.get_m2_path(path_to_m2_directory))[0])\n",
    "            \n",
    "    vdict = sb.collect_vulnerabilities(spotbugs_xml, {'uv': project_classes, 'dv': dep_classes})\n",
    "    \n",
    "    uv_classes = [sb.get_main_classname(b) for c in vdict['uv'] for r in c for b in r]\n",
    "    uv_classes = list(set(uv_classes))\n",
    "    \n",
    "    dv_classes = [sb.get_main_classname(b) for c in vdict['dv'] for r in c for b in r]\n",
    "    dv_classes = list(set(dv_classes))\n",
    "    \n",
    "    uv_count = [len(r) for c in vdict['uv'] for r in c]\n",
    "    dv_count = [len(r) for c in vdict['dv'] for r in c]\n",
    "    \n",
    "    u_sloc, d_sloc, uv_classes_sloc, dv_classes_sloc = 0, 0, 0, 0 #angor\n",
    "    \n",
    "    try:#angor\n",
    "        u_sloc = sum([int(classes_sloc.get(c,0)) for c in sloc.get_roots(project_classes)])\n",
    "        d_sloc = sum([int(classes_sloc.get(c,0)) for c in sloc.get_roots(dep_classes)])\n",
    "        \n",
    "        uv_classes_sloc = sum([int(classes_sloc.get(c,0)) for c in sloc.get_roots(uv_classes)])\n",
    "        dv_classes_sloc = sum([int(classes_sloc.get(c,0)) for c in sloc.get_roots(dv_classes)])\n",
    "    except Exception as e:#angor\n",
    "        with open ('/home/agkortzis/git_repos/ICSR19/analysis/log_dep_classes.txt', 'w') as log:\n",
    "            for entry in sloc.get_roots(dep_classes):\n",
    "                log.write(\"{}\\n\".format(entry))\n",
    "            \n",
    "        with open ('/home/agkortzis/git_repos/ICSR19/analysis/log_dep_original.txt', 'w') as  log:\n",
    "            for entry in dep_classes:\n",
    "                log.write(\"{}\\n\".format(entry))\n",
    "            \n",
    "        raise e#angor\n",
    "        logging.error(\"Error while calculating metrics\\n{}\".format(e))#angor\n",
    "    \n",
    "    return [\n",
    "        len(project_classes),  # #u_classes   \n",
    "        len(dep_classes),      # #d_classes \n",
    "        len(uv_classes),       # #uv_classes \n",
    "        len(dv_classes),       # #dv_classes \n",
    "        u_sloc,                # #u_sloc \n",
    "        d_sloc,                # #d_sloc  \n",
    "        uv_classes_sloc ,      # #uv_classes_sloc \n",
    "        dv_classes_sloc        # #dv_classes_sloc \n",
    "    ] + uv_count + dv_count    # #uv_p1_r1 | #uv_p1_r2 ... | #dv_p3_r3 | #dv_p3_r4\n",
    "\n",
    "    \n",
    "def collect_sp_metrics(file_project_trees, output_file, append_to_file=True, path_to_m2_directory=os.path.expanduser('~/.m2')):\n",
    "\n",
    "    trees = mvn.get_compiled_modules(file_project_trees)\n",
    "    spotbugs_xml = f'{os.path.splitext(file_project_trees)[0]}.xml'\n",
    "    proj_name = os.path.basename(os.path.splitext(file_project_trees)[0])\n",
    "    logging.info(\"Project :: {}\".format(proj_name))\n",
    "    \n",
    "    if not trees:\n",
    "        logging.warning(f'No modules to analyze: {file_project_trees}.')\n",
    "        return\n",
    "    \n",
    "    if not os.path.exists(spotbugs_xml):\n",
    "        logging.warning(f'SpotBugs XML not found: {spotbugs_xml}.')\n",
    "        return\n",
    "    \n",
    "    metrics = project_level_metrics(trees, spotbugs_xml, path_to_m2_directory)\n",
    "    \n",
    "    if append_to_file:\n",
    "        with open(output_file, 'a') as f:\n",
    "            f.write(','.join([proj_name] + [str(m) for m in metrics]) + os.linesep)\n",
    "    else:\n",
    "        with open(output_file, 'w') as f:\n",
    "            f.write(','.join([proj_name] + [str(m) for m in metrics]) + os.linesep)\n",
    "            \n",
    "    logging.debug(','.join([proj_name] + [str(m) for m in metrics]) + os.linesep)\n",
    "\n",
    "currentDT = datetime.datetime.now()\n",
    "print (\"Started at :: {}\".format(str(currentDT)))\n",
    "\n",
    "metrics_header = ['#u_classes', '#d_classes', '#uv_classes', '#dv_classes', \n",
    "           '#u_sloc', '#d_sloc', '#uv_classes_sloc', '#dv_classes_sloc', \n",
    "           '#uv_p1_r1', '#uv_p1_r2', '#uv_p1_r3', '#uv_p1_r4', \n",
    "           '#uv_p2_r1', '#uv_p2_r2', '#uv_p2_r3', '#uv_p2_r4', \n",
    "           '#uv_p3_r1', '#uv_p3_r2', '#uv_p3_r3', '#uv_p3_r4', \n",
    "           '#dv_p1_r1', '#dv_p1_r2', '#dv_p1_r3', '#dv_p1_r4', \n",
    "           '#dv_p2_r1', '#dv_p2_r2', '#dv_p2_r3', '#dv_p2_r4', \n",
    "           '#dv_p3_r1', '#dv_p3_r2', '#dv_p3_r3', '#dv_p3_r4', ]\n",
    "\n",
    "with open(projects_dataset, 'w') as f:\n",
    "    f.write(','.join((['project'] + metrics_header)) + os.linesep)\n",
    "    \n",
    "projects_tress = [f for f in os.listdir(path_to_data) if f.endswith('.trees')]\n",
    "\n",
    "number_of_projects = len(projects_tress)\n",
    "for index, f in enumerate(projects_tress):\n",
    "    logging.info(\"{}/{} --> {}\".format(index,number_of_projects,f))\n",
    "    filepath = path_to_data + os.path.sep + f\n",
    "    collect_sp_metrics(filepath, projects_dataset, path_to_m2_directory)\n",
    "\n",
    "currentDT = datetime.datetime.now()\n",
    "print (\"Finished at :: {}\".format(str(currentDT)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
